{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the output of the random walks is a single set of pages. End users will prefer a ranked list of pages. This ranking should have a tendency to rank pages from the target WUJ higher than pages not in that WUJ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking by page frequency-random walk frequency\n",
    "\n",
    "#### An example\n",
    "\n",
    "The most successful random walk method has been to perform multiple random walks and combine the pages visited by each one into a single set of pages.\n",
    "\n",
    "Each random walk traverses a path of pages. Since we perform multiple random walks, we have multiple paths. Some pages will appear on more paths than others. Some pages will appear more frequently per path.\n",
    "\n",
    "For example: suppose you perform two random walks and each one traverses the following path:\n",
    "\n",
    "- [A, C, D, C, X, Y, Z] \n",
    "- [A, C, B, D, Q, P, M]\n",
    "\n",
    "Pages A, C and D are common to both paths. However, C occurs twice on the first path, which no other page does. Hence, C should be ranked first, followed by A and D in joint second. The remaining pages are equally ranked at the bottom.\n",
    "\n",
    "#### Page frequency-path frequency\n",
    "\n",
    "Inspired by the tf-idf (\"term frequency-inverse document frequency\") metric from NLP, we create the tf-df metric, \"term frequency-document frequency\". Translated into random walk parlance, this is \"page frequency-path frequency\". Where \"page frequency\" is the number of occurences of a given page on a given path taken by a random walk, and, \"path frequency\" is the number of random walk paths on which a given page occurs at least once.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$pf(r,p)$ is the frequency of page $p$ on a single random walk $r$,\n",
    "\n",
    "$$\\text{pf}(r,p) = f_{r,p}$$\n",
    "\n",
    "Where $f_{r,p}$ is the count of a page on a random walk path.\n",
    "\n",
    "The path frequency is a measure of how common a given page is to all random walks performed, i.e. if it's common or rare across all random walks,\n",
    "\n",
    "$$\\text{rwf}(p,R) = |\\{r \\in R : p \\in r\\}|$$\n",
    "\n",
    "Where $R$ is the set of paths taken by all random walks and $|\\{r \\in R : p \\in r\\}|$ is the number of random walks on which the page $p$ occurs. For instance, in the above example, page C occurs on two random walk paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can compute the page frequency-path frequency metric. We start by constructing an $n\\times m$ matrix, $A$, where $n$ is the number of random walks performed and $m$ is the number of unique pages visited across all random walks.\n",
    "\n",
    "Hence, $A_{r,p}=f_{r,p}$, i.e. the value on the $p$-th column of the $r$-th row is the the number of occurrences of page $p$ on random walk $r$.\n",
    "\n",
    "Then, for each page, we compute the path frequency score $\\text{rwf}(p,R)$. Doing this for each page gives a vector of length $m$, called $v$, and we convert this into a diagonal matrix: $V=\\text{diag}(v)$, where the elements of $v$ occupy the main diagonal.\n",
    "\n",
    "Once we have this, we simply calculate $AV$. This means each $(AV)_{r,p}=f_{r,p} \\text{rwf}(p,R)$.\n",
    "\n",
    "There are multiple ways to proceed from here, but somehow we need to compress $AV$ down into a vector, where we have a score for each page. We could average each column, we could take the max, the min, and so on.\n",
    "\n",
    "In our implementation, each page is ranked by the maximum score for its column in $AV$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a demo is shown of this ranking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.randomwalks as rw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/hjv00zzd3f989l8lnrk7xfnc0000gp/T/ipykernel_6720/2686123259.py:20: DeprecationWarning: adj_matrix is deprecated and will be removed in version 3.0.\n",
      "Use `adjacency_matrix` instead\n",
      "\n",
      "  A = nx.adj_matrix(G, weight=None)\n"
     ]
    }
   ],
   "source": [
    "# er_pages is a list of pages known to be within the economic recovery WUJ\n",
    "# this will be used to help evaluate the ranking system\n",
    "\n",
    "er_pages = pd.read_excel('../../data/processed/2021-11-12 - Economic recovery pages.xlsx', sheet_name='Top pages').pagePathv2.to_list()\n",
    "\n",
    "# get networkx graph\n",
    "G = nx.read_gpickle(\"../../data/processed/functional_session_hit_directed_graph_er.gpickle\").to_undirected()\n",
    "\n",
    "# reformat the graph to make it compliant with existing random walk functions\n",
    "# i.e. add the path to a name property and set the index to be a number\n",
    "\n",
    "for index,data in G.nodes(data=True):\n",
    "    data['properties'] = dict()\n",
    "    data['properties']['name'] = index\n",
    "\n",
    "\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default', label_attribute=None)\n",
    "\n",
    "# get adjacency matrix of G\n",
    "A = nx.adj_matrix(G, weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seeds from where random walks will be initialised\n",
    "seeds = (\n",
    "    '/find-a-job',\n",
    "    '/universal-credit',\n",
    "    '/government/collections/financial-support-for-businesses-during-coronavirus-covid-19'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32feb4b1a4e7453293781ce7f7cec879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = rw.repeat_random_walks(steps=100, repeats=100, T=A, G=G, seed_pages=seeds, proba=False, combine='union', level=1, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_scores = rw.page_freq_path_freq_ranking(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_scores['ER'] = page_scores.pagePath.isin(er_pages)\n",
    "colour = (page_scores.ER == True).map({True: 'background-color: black', False: ''})\n",
    "page_scores = page_scores.style.apply(lambda s: colour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns to output\n",
    "\n",
    "Add additional information to csv output: \n",
    "- document type\n",
    "- document super type\n",
    "- number of sessions that visit this page\n",
    "- number of sessions where this page is an entrance hit\n",
    "- number of sessions where this page is an exit hit\n",
    "- number of sessions where this page is both an entrance and exit hit\n",
    "- how frequent the page occurs in the whole user journey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with `pagePath`: `documentType`, `sessionHitsAll`, `entranceHit`, `exitHit`, `entranceAndExitHit`\n",
    "df_dict = {info['properties']['name']: [info['documentType'], info['sessionHitsAll'], info['entranceHit'], info['exitHit'], info['entranceAndExitHit']] for node, info in G.nodes(data=True)}\n",
    "df_dict = {k:v for (k,v) in df_dict.items() if k in page_scores['pagePath'].tolist()}\n",
    "df_info = pd.DataFrame.from_dict(df_dict, orient='index', columns=['documentType', 'sessionHitsAll', 'entranceHit', 'exitHit', 'entranceAndExitHit']).rename_axis('pagePath').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with document supertypes\n",
    "news_and_comms_doctypes = {'medical_safety_alert', 'drug_safety_update', 'news_article', \n",
    "                           'news_story', 'press_release', 'world_location_news_article', \n",
    "                           'world_news_story', 'fatality_notice', 'fatality_notice', \n",
    "                           'tax_tribunal_decision', 'utaac_decision', 'asylum_support_decision', \n",
    "                           'employment_appeal_tribunal_decision', 'employment_tribunal_decision', \n",
    "                           'employment_tribunal_decision', 'service_standard_report', 'cma_case', \n",
    "                           'decision', 'oral_statement', 'written_statement', 'authored_article', \n",
    "                           'correspondence', 'speech', 'government_response', 'case_study' \n",
    "}\n",
    "\n",
    "service_doctypes = {'completed_transaction', 'local_transaction', 'form', 'calculator',\n",
    "                    'smart_answer', 'simple_smart_answer', 'place', 'licence', 'step_by_step_nav', \n",
    "                    'transaction', 'answer', 'guide'\n",
    "}\n",
    "\n",
    "guidance_and_reg_doctypes = {'regulation', 'detailed_guide', 'manual', 'manual_section',\n",
    "                             'guidance', 'map', 'calendar', 'statutory_guidance', 'notice',\n",
    "                             'international_treaty', 'travel_advice', 'promotional', \n",
    "                             'international_development_fund', 'countryside_stewardship_grant',\n",
    "                             'esi_fund', 'business_finance_support_scheme', 'statutory_instrument',\n",
    "                             'hmrc_manual', 'standard'\n",
    "}\n",
    "\n",
    "policy_and_engage_doctypes = {'impact_assessment', 'policy_paper', 'open_consultation',\n",
    "                              'policy_paper', 'closed_consultation', 'consultation_outcome',\n",
    "                              'policy_and_engagement'  \n",
    "}\n",
    "\n",
    "research_and_stats_doctypes = {'dfid_research_output', 'independent_report', 'research', \n",
    "                               'statistics', 'national_statistics', 'statistics_announcement',\n",
    "                               'national_statistics_announcement', 'official_statistics_announcement',\n",
    "                               'statistical_data_set', 'official_statistics'\n",
    "}\n",
    "\n",
    "transparency_doctypes = {'transparency', 'corporate_report', 'foi_release', 'aaib_report',\n",
    "                         'raib_report', 'maib_report'\n",
    "}\n",
    "\n",
    "document_type_dict = dict.fromkeys(list(set(df_info['documentType'])))\n",
    "\n",
    "for docType, docSupertype in document_type_dict.items():\n",
    "    if docType in news_and_comms_doctypes: \n",
    "        document_type_dict[docType] = 'news and communication'\n",
    "    \n",
    "    elif docType in service_doctypes:\n",
    "        document_type_dict[docType] = 'services'\n",
    "    \n",
    "    elif docType in guidance_and_reg_doctypes:\n",
    "        document_type_dict[docType] = 'guidance and regulation'\n",
    " \n",
    "    elif docType in policy_and_engage_doctypes:\n",
    "        document_type_dict[docType] = 'policy and engagement'\n",
    "    \n",
    "    elif docType in research_and_stats_doctypes:\n",
    "        document_type_dict[docType] = 'research and statistics'\n",
    "    \n",
    "    elif docType in transparency_doctypes:\n",
    "        document_type_dict[docType] = 'transparency'\n",
    "    \n",
    "    else: \n",
    "        document_type_dict[docType] = 'other' \n",
    "\n",
    "df_docSuper = pd.DataFrame(document_type_dict.items(), columns=['documentType', 'documentSupertype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs \n",
    "df_merged = pd.merge(page_scores, df_info, on='pagePath')\n",
    "df_merged = pd.merge(df_merged, df_docSuper, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reoder and rename df columns \n",
    "df_merged = df_merged[['pagePath', 'documentType', 'documentSupertype', 'sessionHitsAll', 'entranceHit', 'exitHit', 'entranceAndExitHit', 'tfdf_max']]\n",
    "df_merged = df_merged.rename(columns={'pagePath': 'page path', 'documentType': 'document type', 'documentSupertype': 'document supertype', 'sessionHitsAll': 'number of sessions that visit this page', 'entranceHit': 'number of sessions where this page is an entrance hit', 'exitHit': 'number of sessions where this page is an exit hit', 'entranceAndExitHit': 'number of sessions where this page is both an entrance and exit hit', 'tfdf_max': 'how frequent the page occurs in the whole user journey'})\n",
    "\n",
    "# save df\n",
    "df_merged.to_csv('../../data/processed/pages_ranked_with_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8b846935c606cea246de54deed5846dcd1684508b0f7522a02f36eb958d20d0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
